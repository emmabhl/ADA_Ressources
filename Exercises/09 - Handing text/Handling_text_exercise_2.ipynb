{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from helpers.helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling text 2 exercise\n",
    "[Handling text exercisses ADApted drom ADA 2018 final exam]\n",
    "\n",
    "The Sheldon Cooper we all know and love (OK, some of us might not know him, and some might not love him) from the TV series \"The Big Bang Theory\" has gotten into an argument with Leonard from the same TV show. Sheldon insists that he knows the show better than anyone, and keeps making various claims about the show, which neither of them know how to prove or disprove. The two of them have reached out to you ladies and gentlemen, as data scientists, to help them. You will be given the full script of the series, with information on the episode, the scene, the person saying each dialogue line, and the dialogue lines themselves.\n",
    "\n",
    "Leonard has challenged several of Sheldon's claims about the show, and throughout this exam you will see some of those and you will get to prove or disprove them, but remember: sometimes, we can neither prove a claim, nor disprove it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A: Picking up the shovel\n",
    "\n",
    "**Note: You will use the data you preprocess in this task in all the subsequent ones.**\n",
    "\n",
    "Our friends' argument concerns the entire show. We have given you a file in the `data/` folder that contains the script of every single episode. New episodes are indicated by '>>', new scenes by '>', and the rest of the lines are dialogue lines. Some lines are said by multiple people (for example, lines indicated by 'All' or 'Together'); **you must discard these lines**, for the sake of simplicity. However, you do not need to do it for Q1 in this task -- you'll take care of it when you solve Q2.\n",
    "\n",
    "**Q1**. Your first task is to extract all lines of dialogue in each scene and episode, creating a dataframe where each row has the episode and scene where a dialogue line was said, the character who said it, and the line itself. You do not need to extract the proper name of the episode (e.g. episode 1 can appear as \"Series 01 Episode 01 - Pilot Episode\", and doesn't need to appear as \"Pilot Episode\"). Then, answer the following question: In total, how many scenes are there in each season? We're not asking about unique scenes; the same location appearing in two episodes counts as two scenes. You can use a Pandas dataframe with a season column and a scene count column as the response.\n",
    "\n",
    "**Note: The data refers to seasons as \"series\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Scene</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>So if a photon is directed through a plane wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Agreed, what’s your point?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>There’s no point, I just think it’s a good id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Excuse me?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Hang on.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51287</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>Ramona</td>\n",
       "      <td>Mmm. No big deal, I enjoy spending time with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51288</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>And I with you. Question, are you seeking a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51289</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>Ramona</td>\n",
       "      <td>What if I were?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51290</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51291</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Princeton.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>(Knock, knock, knock) Amy. (Knock, knock, kno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51292 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Series Episode                        Scene     Character  \\\n",
       "0         01      01  A corridor at a sperm bank.       Sheldon   \n",
       "1         01      01  A corridor at a sperm bank.       Leonard   \n",
       "2         01      01  A corridor at a sperm bank.       Sheldon   \n",
       "3         01      01  A corridor at a sperm bank.       Leonard   \n",
       "4         01      01  A corridor at a sperm bank.  Receptionist   \n",
       "...      ...     ...                          ...           ...   \n",
       "51287     10      24            Sheldon’s office.        Ramona   \n",
       "51288     10      24            Sheldon’s office.       Sheldon   \n",
       "51289     10      24            Sheldon’s office.        Ramona   \n",
       "51290     10      24            Sheldon’s office.       Sheldon   \n",
       "51291     10      24                   Princeton.       Sheldon   \n",
       "\n",
       "                                                    Line  \n",
       "0       So if a photon is directed through a plane wi...  \n",
       "1                             Agreed, what’s your point?  \n",
       "2       There’s no point, I just think it’s a good id...  \n",
       "3                                             Excuse me?  \n",
       "4                                               Hang on.  \n",
       "...                                                  ...  \n",
       "51287   Mmm. No big deal, I enjoy spending time with ...  \n",
       "51288   And I with you. Question, are you seeking a r...  \n",
       "51289                                    What if I were?  \n",
       "51290   Well, that would raise a number of problems. ...  \n",
       "51291   (Knock, knock, knock) Amy. (Knock, knock, kno...  \n",
       "\n",
       "[51292 rows x 5 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/all_scripts.txt'\n",
    "\n",
    "# Read the file into a list of lines\n",
    "with open(path, 'r', encoding = 'utf8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Create a list of dictionaries, where each dictionary represents a record\n",
    "df = pd.DataFrame(columns=['Series', 'Episode', 'Scene', 'Character', 'Line'])\n",
    "\n",
    "# Iterate through each line, current_entry = one log entry with all columns, df = list of all votee/voter pairs\n",
    "for line in lines:\n",
    "    line = line.strip('\\n').strip()\n",
    "    if line.startswith('>> '):\n",
    "        _, serie, episode_nb, _ = re.split('>> Series | Episode | – ', string=line)\n",
    "        continue\n",
    "    elif line.startswith('> '):\n",
    "        scene = line[2:]\n",
    "        continue\n",
    "    else :\n",
    "        try:\n",
    "            charact, dialogue = line.split(':', 1)\n",
    "        except:\n",
    "            print(line)\n",
    "        df.loc[len(df)] = ({'Series': serie, 'Episode': episode_nb, 'Scene': scene,'Character': charact, 'Line': dialogue})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scenes in each season:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series\n",
       "01    157\n",
       "02    205\n",
       "03    193\n",
       "04    219\n",
       "05    198\n",
       "06    211\n",
       "07    207\n",
       "08    191\n",
       "09    177\n",
       "10    189\n",
       "Name: Scene, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of scenes in each season:')\n",
    "df.groupby(['Series', 'Episode', 'Scene']).Scene.first().groupby('Series').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Now, let's define two sets of characters: all the characters, and recurrent characters. Recurrent characters are those who appear in more than one episode. For the subsequent sections, you will need to have a list of recurrent characters. Assume that there are no two _named characters_ (i.e. characters who have actual names and aren't referred to generically as \"little girl\", \"grumpy grandpa\", etc.) with the same name, i.e. there are no two Sheldons, etc. Generate a list of recurrent characters who have more than 90 dialogue lines in total, and then take a look at the list you have. If you've done this correctly, you should have a list of 20 names. However, one of these is clearly not a recurrent character. Manually remove that one, and print out your list of recurrent characters. To remove that character, pay attention to the _named character_ assumption we gave you earlier on. **For all the subsequent questions, you must only keep the dialogue lines said by the recurrent characters in your list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of line total: 51292\n",
      "Number of line without all and together: 51185\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of line total: {len(df)}')\n",
    "df = df[~df.Character.isin(['All', 'Together'])]\n",
    "print(f'Number of line without all and together: {len(df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reccurent characters: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Sheldon', 'Leonard', 'Penny', 'Howard', 'Raj', 'Mrs Cooper',\n",
       "       'Leslie', 'Kripke', 'Beverley', 'Stuart', 'Bernadette', 'Wil',\n",
       "       'Mrs Wolowitz', 'Zack', 'Amy', 'Priya', 'Arthur', 'Bert', 'Emily'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rec_char = df.groupby('Character').agg({('n_episode', lambda x: x.nunique()), ('n_dialogue', lambda x: x.size)}).Episode\n",
    "rec_char = rec_char[(rec_char.n_episode > 1) & (rec_char.n_dialogue > 90)].index\n",
    "print(f'Number of reccurent characters: {rec_char.size}')\n",
    "rec_char = rec_char[rec_char != 'Man']\n",
    "\n",
    "df_rec = df[df.Character.isin(rec_char)]\n",
    "display(df_rec.Character.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Read the scripts carefully\n",
    "\n",
    "### Part 1: Don't put the shovel down just yet\n",
    "\n",
    "**Q3**. From each dialogue line, replace punctuation marks (listed in the EXCLUDE_CHARS variable provided in `helpers/helper_functions.py`) with whitespaces, and lowercase all the text. **Do not remove any stopwords, leave them be for all the questions in this task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line:str):\n",
    "    for element in EXCLUDE_CHARS:\n",
    "        line = line.replace(element, ' ')\n",
    "    return line.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6p/skl5q4mn4kqcwg0hbd513bqr0000gn/T/ipykernel_19196/2892075834.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rec.Line = df_rec.Line.apply(clean_line)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>so if a photon is directed through a plane wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>agreed  what s your point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>there s no point  i just think it s a good id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>excuse me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>one across is aegean  eight down is nabakov  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51284</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>uh  breakfast yes  lunch no  i did have a cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51286</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>how thoughtful  thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51288</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>and i with you  question  are you seeking a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51290</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>well  that would raise a number of problems  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51291</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>knock  knock  knock  amy   knock  knock  kno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48346 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Series Episode Character  \\\n",
       "0         01      01   Sheldon   \n",
       "1         01      01   Leonard   \n",
       "2         01      01   Sheldon   \n",
       "3         01      01   Leonard   \n",
       "5         01      01   Leonard   \n",
       "...      ...     ...       ...   \n",
       "51284     10      24   Sheldon   \n",
       "51286     10      24   Sheldon   \n",
       "51288     10      24   Sheldon   \n",
       "51290     10      24   Sheldon   \n",
       "51291     10      24   Sheldon   \n",
       "\n",
       "                                                    Line  \n",
       "0       so if a photon is directed through a plane wi...  \n",
       "1                             agreed  what s your point   \n",
       "2       there s no point  i just think it s a good id...  \n",
       "3                                             excuse me   \n",
       "5       one across is aegean  eight down is nabakov  ...  \n",
       "...                                                  ...  \n",
       "51284   uh  breakfast yes  lunch no  i did have a cou...  \n",
       "51286                         how thoughtful  thank you   \n",
       "51288   and i with you  question  are you seeking a r...  \n",
       "51290   well  that would raise a number of problems  ...  \n",
       "51291    knock  knock  knock  amy   knock  knock  kno...  \n",
       "\n",
       "[48346 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec.Line = df_rec.Line.apply(clean_line)\n",
    "df_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**. For each term, calculate its \"corpus frequency\", i.e. its number of occurrences in the entire series. Visualize the distribution of corpus frequency using a histogram. Explain your observations. What are the appropriate x and y scales for this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "count = vectorizer.fit_transform(df_rec.Line)\n",
    "corpus_freq = count.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20484"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out().size\n",
    "#np.ravel(corpus_freq).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525944"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_by_words = pd.DataFrame({'word': vectorizer.get_feature_names_out() , 'freq': np.ravel(corpus_freq)})\n",
    "freqs_by_words.freq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.log(np.ravel(corpus_freq))\n",
    "plt.hist(x=data, bins=200, log=True)\n",
    "plt.xticks(ticks=np.arange(np.min(data), np.max(data), 2), labels=[f'{10**x:.0e}'.format(x) for x in np.arange(11, step=2)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Talkativity\n",
    "**Q5**. For each of the recurrent characters, calculate their total number of words uttered across all episodes. Based on this, who seems to be the most talkative character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Character</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sheldon</th>\n",
       "      <td>121018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leonard</th>\n",
       "      <td>102459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Penny</th>\n",
       "      <td>77651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Howard</th>\n",
       "      <td>59045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raj</th>\n",
       "      <td>48884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amy</th>\n",
       "      <td>34258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernadette</th>\n",
       "      <td>26942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stuart</th>\n",
       "      <td>7038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Priya</th>\n",
       "      <td>2619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Cooper</th>\n",
       "      <td>2257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beverley</th>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td>1619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Wolowitz</th>\n",
       "      <td>1502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wil</th>\n",
       "      <td>1402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arthur</th>\n",
       "      <td>1361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kripke</th>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zack</th>\n",
       "      <td>1165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leslie</th>\n",
       "      <td>1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert</th>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "Character           \n",
       "Sheldon       121018\n",
       "Leonard       102459\n",
       "Penny          77651\n",
       "Howard         59045\n",
       "Raj            48884\n",
       "Amy            34258\n",
       "Bernadette     26942\n",
       "Stuart          7038\n",
       "Priya           2619\n",
       "Mrs Cooper      2257\n",
       "Beverley        1865\n",
       "Emily           1619\n",
       "Mrs Wolowitz    1502\n",
       "Wil             1402\n",
       "Arthur          1361\n",
       "Kripke          1223\n",
       "Zack            1165\n",
       "Leslie          1011\n",
       "Bert             619"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words = pd.DataFrame(count.sum(axis = 1))\n",
    "nb_words['Character'] = df_rec.Character\n",
    "nb_words.groupby('Character').sum().sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D: The Detective's Hat\n",
    "\n",
    "Sheldon claims that given a dialogue line, he can, with an accuracy of above 70%, say whether it's by himself or by someone else. Leonard contests this claim, since he believes that this claimed accuracy is too high.\n",
    "\n",
    "**Q6**. Divide the set of all dialogue lines into two subsets: the training set, consisting of all the seasons except the last two, and the test set, consisting of the last two seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01', '02', '03', '04', '05', '06', '07', '08', '09', '10'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "all_season = df_rec.Series.unique()\n",
    "display(all_season)\n",
    "train_df = df_rec[df_rec.Series.isin(all_season[:-2])]\n",
    "test_df = df_rec[df_rec.Series.isin(all_season[-2:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**. Find the set of all words in the training set that are only uttered by Sheldon. Is it possible for Sheldon to identify himself only based on these? Use the test set to assess this possibility, and explain your method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_train = train_df.Character == 'Sheldon'\n",
    "out_test = test_df.Character == 'Sheldon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000lb', '01100111', '0700', ..., 'zur', 'zzzzzzzzz', 'être'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(train_df[out_train].Line)\n",
    "Sheldon_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(train_df[~out_train].Line)\n",
    "Other_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "Sheldon_unique = [i for i in Sheldon_words if i not in Other_words]\n",
    "np.array(Sheldon_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7800177008555413"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_Sheldon = test_df.Line.apply(lambda x: any(element in Sheldon_unique for element in x)) \n",
    "accuracy = (pred_Sheldon == out_test).sum()/len(out_test)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
